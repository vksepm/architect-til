# **Jupyter Notebooks for Learning Large Language Model Security Vulnerabilities**

1. Introduction:
   The increasing deployment of Large Language Models (LLMs) across a multitude of applications has brought forth the critical need to understand and address their inherent security vulnerabilities. These vulnerabilities, if left unmitigated, can lead to severe consequences, ranging from the generation of harmful content to the compromise of sensitive data and the undermining of application integrity. Jupyter notebooks have emerged as a powerful and versatile tool for exploring complex technical subjects, providing an interactive environment where theoretical knowledge can be combined with practical experimentation through code execution and visualization. This report aims to systematically investigate the availability of Jupyter notebook resources that serve as educational materials for understanding and demonstrating key security vulnerabilities in LLMs. These vulnerabilities encompass prompt hacking, which includes techniques like prompt injection, data and prompt leaking, and jailbreaking, as well as backdoor attacks, which can be introduced through training data poisoning and activated by specific triggers. Furthermore, the report will explore resources related to defensive measures, such as red teaming methodologies, the use of specialized security scanning frameworks like Garak, and the critical aspect of monitoring LLM applications in production for potential security breaches. The availability of accessible and hands-on learning materials, particularly in the form of Jupyter notebooks, is crucial for empowering researchers, developers, and security professionals to effectively identify, analyze, and ultimately mitigate the security risks associated with the widespread adoption of LLMs \[Insight 1\]. The diverse nature of these vulnerabilities necessitates a structured and comprehensive approach to learning and defense, as highlighted by the various categories outlined in this report \[Insight 2\]. By providing a detailed overview of the existing Jupyter notebook resources, this report seeks to guide individuals towards practical learning opportunities in the rapidly evolving domain of LLM security.

2. Understanding Prompt Injection Techniques through Jupyter Notebooks:
   Prompt injection attacks represent a significant category of vulnerabilities in LLMs, wherein carefully crafted adversarial prompts can manipulate the model to deviate from its intended behavior.1 This manipulation can range from causing the model to ignore original instructions to eliciting unintended actions or the disclosure of confidential information. Prompt injection can be broadly categorized into direct injection, where the malicious instructions are embedded directly within the user's prompt, and indirect injection, where the malicious content is introduced through external data sources that the LLM processes.2 Several Jupyter notebook resources offer avenues for understanding and exploring these techniques. The repository github.com/sinanw/llm-security-prompt-injection contains a series of notebooks that analyze prompt injection from a classification perspective.3 While these notebooks do not directly demonstrate the techniques of prompt injection, they provide a valuable foundation by exploring how machine learning, including traditional methods and fine-tuned LLMs, can be used to classify prompts as potentially malicious. For instance, the notebook ml-classification.ipynb investigates the use of traditional machine learning algorithms for binary classification of prompts to identify malicious inputs, achieving an accuracy of 96.55% with Logistic Regression.4 This demonstrates the feasibility of using established machine learning techniques to distinguish between benign and potentially harmful prompts. Furthermore, the repository includes notebooks that explore the performance of pre-trained and fine-tuned LLMs (specifically XLM-RoBERTa) on this classification task, showing an evolution in detection approaches.4 The repository github.com/greshake/llm-security offers demonstrations of indirect prompt injection attacks, and given that Jupyter Notebook is a primary language used in this repository (80.2%), it is highly probable that these demonstrations are implemented within notebooks.2 These demonstrations showcase various attack vectors, including the ability to gain remote control over LLMs, leak sensitive user data, establish persistent compromise across sessions, spread injections to other LLMs, and even target code completion engines.2 One compelling example, "Ask for Einstein, get Pirate," illustrates how a small, hidden injection within a seemingly innocuous piece of content retrieved by the LLM can completely alter its persona.2 Another scenario, "Spreading injections via E-Mail," highlights the risk of LLMs being used in automated workflows where a compromised agent can propagate malicious instructions.2 These examples underscore the significant potential impact of prompt injection, extending beyond mere unintended responses to severe security breaches. The HouYi framework, available at github.com/LLMSecurity/HouYi, provides tools for automating prompt injection attacks.5 While the repository lists Jupyter Notebook as a language used (11.3%), the primary examples are provided as Python scripts.5 The framework allows users to define harnesses to interact with LLM-integrated applications and specify their attack intentions, such as appending "Pwned\!\!" to a translator's response.5 This framework represents a step towards the automation of LLM vulnerability testing, including prompt injection \[Insight 1\]. The fact that HouYi is built upon GPT models indicates the reliance on the capabilities of advanced LLMs in both attacking and defending against such vulnerabilities \[Insight 2\]. Additionally, the repository github.com/Avmb/adversarial\_MT\_prompt\_injection focuses on prompt injection specifically within machine translation systems, demonstrating techniques to make the system ignore translation instructions and instead answer arbitrary questions.1 While the format of the code is not explicitly stated, it highlights that prompt injection vulnerabilities are not limited to general-purpose LLMs but can also affect specialized NLP applications \[Insight 1\]. The techniques explored, such as prepending adversarial prompts, offer concrete examples of how these attacks can be crafted \[Insight 2\]. In summary, Jupyter notebooks serve as a valuable medium for both analyzing the characteristics of prompt injection attacks and demonstrating the practical methods by which these attacks can be carried out, covering a range of scenarios from direct manipulation to more sophisticated indirect techniques.  
3. Exploring Data and Prompt Leaking Vulnerabilities with Jupyter Notebooks:  
   Data and prompt leaking vulnerabilities in LLMs involve the unintended disclosure of sensitive information that might be embedded within the model's system prompts or training data.6 The risks associated with such leakage are substantial, potentially exposing confidential data, intellectual property, or internal system architectures, which can then be exploited to facilitate further attacks. Several Jupyter notebook-based resources are available for investigating these vulnerabilities. The project github.com/sternakt/prompt-leakage-probing offers a set of three Jupyter notebooks designed to test the susceptibility of LLM agents to system prompt leaks.7 These notebooks (named low\_protection.ipynb, medium\_protection.ipynb, and high\_protection.ipynb) assess the strength of prompt leakage protection by calculating a "judge advantage," which measures how easily an external "judge" can distinguish between an agent using the original system prompt and one using a sanitized version.7 This research aims to quantify the vulnerability of LLMs to prompt leakage under varying levels of defense \[Insight 1\]. The "judge advantage" metric provides a means to evaluate the effectiveness of different sanitization techniques \[Insight 2\]. The repository github.com/salesforce/prompt-leakage provides data and code (primarily in Python, suggesting potential use in notebooks) for experiments focused on prompt leakage, particularly in the context of multi-turn LLM interactions.8 This research investigates LLM vulnerabilities across diverse domains such as news, legal, finance, and medical, and evaluates the efficacy of various defense strategies against prompt leakage.8 The emphasis on multi-turn interactions highlights the added complexity of ensuring prompt security in conversational settings \[Insight 1\]. Furthermore, the research includes an evaluation of different defense mechanisms and a cost analysis, providing practical insights into mitigating these risks \[Insight 2\]. The github.com/GoogleCloudPlatform/generative-ai repository contains a Jupyter notebook (gemini\_prompt\_attacks\_mitigation\_examples.ipynb) that addresses prompt protection and includes examples of data leaking with transformations.9 The inclusion of data leaking as a topic within prompt attack mitigation suggests its importance in responsible AI development \[Insight 1\]. The mention of "data leaking with transformations" indicates that attackers might employ sophisticated methods to extract data, necessitating advanced detection techniques \[Insight 2\]. While not a Jupyter notebook, the OWASP resource on system prompt leakage (github.com/OWASP/www-project-top-10-for-large-language-model-applications/blob/main/2\_0\_vulns/LLM07\_SystemPromptLeakage.md) provides crucial context by detailing the risks associated with system prompt leakage and offering valuable prevention and mitigation strategies.6 OWASP's recognition of this vulnerability underscores its significance in real-world LLM applications \[Insight 1\], and their recommended mitigation strategies offer practical guidance for developers \[Insight 2\]. Collectively, these resources demonstrate that Jupyter notebooks are actively used to explore and analyze the nuances of data and prompt leaking vulnerabilities in LLMs, examining different protection mechanisms and the complexities introduced by multi-turn interactions.  
4. **Practical Examples of Jailbreaking LLMs in Jupyter Notebooks:**  
   Jailbreaking LLMs refers to the technique of crafting prompts that can bypass the safety features and ethical guidelines embedded within these models, leading them to generate responses that they are otherwise programmed to avoid.10 The objectives of such attacks can range from eliciting harmful content or accessing restricted information to manipulating the LLM to perform unintended or malicious actions. Several Jupyter notebook-based resources provide practical examples of jailbreaking techniques. The FuzzyAI repository (github.com/cyberark/FuzzyAI) offers a framework specifically designed for jailbreaking LLMs, which includes interactive Jupyter notebooks located under resources/notebooks/.10 This framework supports various attacking methods, such as "tax" (using persuasive language strategies) and "dan" (likely referring to "Do Anything Now" prompts), and is compatible with both locally run LLMs (via Ollama) and cloud-based models like GPT-4o.10 FuzzyAI provides a practical platform for users to experiment with different jailbreaking techniques and understand the vulnerabilities of various LLM architectures \[Insight 1\]. The framework's accessibility is enhanced by offering both a graphical user interface (GUI) and a command-line interface (CLI) \[Insight 2\]. The repository github.com/patrickrchao/JailbreakingLLMs introduces PAIR (Prompt Automatic Iterative Refinement), an algorithm that uses an attacker LLM to automatically generate jailbreaks for a target LLM.13 While the repository doesn't explicitly provide example Jupyter notebooks, the code for running experiments is available and could potentially be adapted into a notebook environment. PAIR represents an advancement in automating the discovery of jailbreaking prompts \[Insight 1\], and its effectiveness against a range of LLMs, including GPT-3.5/4, Vicuna, and GeminiPro-2, highlights the generalizability of this approach \[Insight 2\]. The JailbreakBench repository (github.com/JailbreakBench/jailbreakbench) serves as an open-source benchmark for evaluating the robustness of LLMs against jailbreaking attacks.14 It includes datasets, a pipeline for red-teaming, and implementations of various defense algorithms. Although the primary focus is on benchmarking, the repository may contain Jupyter notebooks for demonstrating the evaluation process or showcasing specific attacks and defenses. JailbreakBench offers a standardized framework for comparing the effectiveness of different jailbreaking techniques and countermeasures \[Insight 1\], encouraging the development of more resilient LLM systems \[Insight 2\]. The github.com/HKUST-KnowComp/LLM-Multistep-Jailbreak repository focuses on more complex, multi-step jailbreak attacks aimed at extracting Personally Identifiable Information (PII).15 While it provides Python scripts for running these attacks, the underlying methodologies could be effectively illustrated in Jupyter notebooks. The repository explores sophisticated techniques like combining jailbreak templates with multi-choice templates and chain-of-thought reasoning to bypass safety features for specific harmful objectives.15 This research indicates that more nuanced and context-aware prompts can be particularly effective in jailbreaking LLMs \[Insight 1\], underscoring the potential privacy risks associated with successful attacks \[Insight 2\]. An article on Medium (medium.com/data-science/exposing-jailbreak-vulnerabilities-in-llm-applications-with-artkit-d2df5f56ece8) discusses jailbreaking vulnerabilities and introduces ARTKIT, a framework for red-teaming LLM applications.11 While specific Jupyter notebooks are not linked in the snippet, the article mentions their use for demonstrations and highlights the importance of testing multi-turn interactions to uncover potential vulnerabilities.11 ARTKIT offers pre-built tools for interacting with popular LLM platforms, simplifying the process of red-teaming \[Insight 2\]. Overall, a variety of resources, including dedicated frameworks and research projects, leverage Jupyter notebooks or provide code that can be used within notebooks to explore and understand the diverse landscape of LLM jailbreaking techniques.  
5. **Demonstrating Backdoor Attacks via Training Data Poisoning in Jupyter Notebooks**:  
   Backdoor attacks in machine learning involve manipulating the training data of a model to embed a hidden trigger. When this trigger is present in an input during inference, it causes the model to misclassify the input in a way that benefits the attacker.16 This can be achieved through training data poisoning, where malicious data samples are injected into the training set. Several Jupyter notebook resources are available that demonstrate these types of attacks. The repository github.com/THUYimingLi/backdoor-learning-resources serves as a comprehensive compilation of research papers and resources on backdoor learning.16 While it mentions the "backdoors101" framework (which has plans for Jupyter notebooks), it does not directly provide notebooks demonstrating training data poisoning.16 However, the extensive list of research highlights the significant academic interest in this area \[Insight 1\], and the mention of various defense mechanisms underscores the ongoing efforts to counter these attacks \[Insight 2\]. The github.com/Trusted-AI/adversarial-robustness-toolbox contains a Jupyter notebook (backdoor\_attack\_DGM.ipynb) that provides a practical demonstration of training time backdoor attacks on Deep Generative Models (DGMs).18 This notebook explores techniques like TrAIL (Training with Adversarial Loss) and ReD (Retraining with Distillation) in the context of a DCGAN trained to generate MNIST-like digits. It clearly illustrates how a specific trigger (a vector of all zeros in this case) can be designed to cause the backdoored generator to produce a target output (an image of a devil icon).18 This provides a concrete, hands-on example of how backdoor attacks can be implemented against generative models \[Insight 1\], and the use of different training techniques showcases various approaches to injecting backdoors \[Insight 2\]. The github.com/bboylyg/BackdoorLLM benchmark focuses on backdoor attacks specifically targeting Large Language Models, with data poisoning attacks (DPAs) being a key strategy.19 While the repository does not explicitly feature Jupyter notebooks, it provides poisoned datasets and Python training scripts for fine-tuning LLMs with backdoors, which could be adapted into notebook format.19 The benchmark covers various attack targets, including sentiment steering, refusal, and jailbreaking, demonstrating the broad applicability of backdoor attacks to LLMs.19 The availability of poisoned datasets allows for direct experimentation with these attacks \[Insight 2\]. The github.com/kohpangwei/data-poisoning-release repository focuses on certified defenses for data poisoning attacks and includes Jupyter notebooks (67.4% of its content).20 While the primary focus is on defenses, the presence of notebooks, such as make\_combined\_paper\_plots.ipynb, suggests their utility in visualizing and analyzing both attack and defense strategies. The emphasis on certified defenses indicates a focus on rigorously verifying the effectiveness of countermeasures \[Insight 1\]. The github.com/THUYimingLi/BackdoorBox toolbox aims to implement a variety of backdoor attacks and defenses.21 While it does not explicitly mention Jupyter notebooks, it provides Python scripts as examples for attacks like BadNets (using a 3x3 white square trigger) and Blended.21 This toolbox aims to provide a unified framework for studying and comparing different backdoor techniques \[Insight 1\], showcasing the diverse ways in which backdoors can be introduced \[Insight 2\]. The github.com/souzatharsis/tamingLLMs/blob/master/tamingllms/notebooks/safety.ipynb notebook discusses LLM safety risks, including dataset poisoning.22 Although the content is not provided in the snippet, its inclusion in a safety-focused notebook underscores the importance of data poisoning as a significant risk factor for LLMs \[Insight 1\]. Finally, the github.com/reds-lab/Narcissus repository provides a Jupyter notebook (Narcissus.ipynb) that demonstrates a clean-label backdoor attack requiring only a few poisoned images to be effective against face recognition datasets.23 The Narcissus attack's ability to achieve high success rates with minimal poisoning and without obvious malicious labels makes it a particularly stealthy and potent threat \[Insight 1\], and the provided notebook makes it easily accessible for study \[Insight 2\]. In conclusion, Jupyter notebooks serve as valuable tools for demonstrating the mechanisms and effectiveness of training data poisoning attacks across various machine learning models, including both discriminative and generative architectures, as well as specifically targeting LLMs.  
6. Illustrating Backdoor Creation and Triggering in Jupyter Notebooks:  
   The process of executing a backdoor attack involves two key stages: first, the creation or embedding of the backdoor within the model, and second, the triggering of this hidden malicious behavior using a specific input pattern or trigger. Jupyter notebooks provide an effective medium for illustrating both of these stages. The Jupyter notebook (backdoor\_attack\_DGM.ipynb) within the github.com/Trusted-AI/adversarial-robustness-toolbox repository not only demonstrates the training phase of a backdoor attack on a DGM but also shows how a specific trigger (a 100-dimensional zero vector) is used to activate the backdoor, causing the model to generate a predefined target output (a devil icon).18 By showing the model's output both with and without the trigger, the notebook clearly illustrates the effect of the backdoor and the mechanism of its activation.18 The Narcissus.ipynb notebook in the github.com/reds-lab/Narcissus repository focuses on a clean-label backdoor attack.23 This notebook includes a function, narcissus\_gen(), which is used to generate the specific trigger for this attack based on the target class data.23 The notebook likely goes on to demonstrate how this generated trigger can then be applied to inputs to manipulate the model's classification in a clean-label manner. The ability of the Narcissus attack to generate triggers using only target class data highlights its stealth and practicality \[Insight 1\], and the notebook simplifies experimentation with this technique \[Insight 2\]. While the examples in the tests sub-folder of the github.com/THUYimingLi/BackdoorBox repository are primarily Python scripts, they illustrate the creation and triggering of backdoors in various attack scenarios.21 For instance, the BadNets example demonstrates the use of a simple 3x3 white square as a trigger to activate the backdoor in a model trained on the CIFAR-10 dataset.21 This highlights that even seemingly innocuous triggers can be effective \[Insight 1\]. Similarly, although the github.com/ebagdasa/backdoors101 framework currently lacks dedicated Jupyter notebooks for defenses, its Python examples for backdoor attacks would likely show how different types of triggers, such as pixel patterns or semantic features, are defined and used to activate the backdoored behavior in the models.24 The framework's support for diverse trigger types underscores the variety of ways in which backdoors can be activated \[Insight 1\]. In essence, Jupyter notebooks and accompanying code examples provide valuable hands-on illustrations of how backdoor triggers are created and subsequently used to activate the hidden malicious functionalities embedded within poisoned machine learning models.  
7. Leveraging Jupyter Notebooks for LLM Red Teaming:  
   Red teaming is a critical security practice that involves simulating adversarial attacks on a system to identify vulnerabilities before they can be exploited by malicious actors.25 In the context of LLMs, red teaming aims to uncover weaknesses in safety mechanisms and other security controls. Jupyter notebooks are increasingly being used to facilitate and demonstrate LLM red-teaming techniques. The DeepTeam framework (github.com/confident-ai/deepteam) is specifically designed for LLM red teaming and offers a "Try Quickstart in Colab" link, suggesting the availability of a Jupyter-like notebook for demonstration.25 DeepTeam incorporates a wide range of pre-defined vulnerabilities and attack methods, making it a comprehensive tool for simulating adversarial scenarios.25 The availability of a quickstart in Colab indicates a focus on making red teaming accessible to users \[Insight 1\]. The github.com/traghav/auto-redteam repository includes a Jupyter Notebook (redteam.ipynb) that demonstrates an innovative approach to LLM red teaming: using one LLM (GPT-4) to generate prompts that are designed to make another LLM (GPT 3.5) say things it is trained not to say.27 This notebook provides a practical example of how AI can be used to automate and potentially enhance the red-teaming process for LLMs \[Insight 1\], and the examples of effective prompts can serve as valuable learning material \[Insight 2\]. The github.com/Improbable-AI/curiosity\_redteam repository presents a novel, curiosity-driven method for training red team LLMs using reinforcement learning to generate diverse and effective test cases.26 While the repository primarily provides Python scripts for experiments, the underlying concepts and methodologies could be effectively explored and demonstrated within Jupyter notebooks. This approach aims to improve the diversity and effectiveness of red-teaming efforts by encouraging the generation of novel test cases.26 Although not directly focused on red-teaming techniques, the github.com/NotSkynet/automated-interpretability-LLM-Red-Teaming repository contains some Jupyter notebooks that explore the interpretability of language models.28 Understanding how LLMs work internally could potentially inform more targeted and effective red-teaming strategies \[Insight 1\]. In summary, Jupyter notebooks are playing an increasingly important role in the field of LLM red teaming, providing platforms for both using specialized frameworks and implementing novel automated techniques to uncover security vulnerabilities.  
8. Utilizing the Garak Framework for Security Testing: Jupyter Notebook Examples:  
   Garak is an open-source framework (github.com/NVIDIA/garak) designed to probe LLMs for a wide range of security vulnerabilities, including hallucination, data leakage, prompt injection, misinformation, toxicity, and jailbreaks.29 It operates by using a variety of probes that send specific prompts to the target LLM and then analyzes the model's responses using detectors to identify potential vulnerabilities. Garak is primarily a command-line tool that can be installed using pip and supports various LLM backends, such as Hugging Face Hub and Replicate.29 While the official documentation and FAQs for Garak do not prominently feature example Jupyter notebooks, the tool's Python-based architecture allows for its integration and use within a notebook environment. For instance, the command-line examples provided in the documentation and in tutorials, such as the Medium article on Garak 30, can be executed directly within a Jupyter notebook using the \! prefix. The Medium article explains useful Garak commands like \--list\_probes to see available vulnerability probes and provides examples of probes like DAN (to bypass restrictions) and Prompt Injection.30 It also demonstrates how to configure Garak to use models from the Hugging Face Hub by setting the API key.30 Garak's ability to systematically scan LLMs for a broad spectrum of vulnerabilities makes it a valuable tool for security testing \[Insight 1\]. Although it is primarily a command-line tool, its integration into Jupyter notebooks allows for more interactive exploration of the results and potentially the customization of probes and detectors \[Insight 2\]. Garak's focus on identifying exploitable weaknesses rather than broader safety concerns is an important distinction for users to understand its specific capabilities.31 While explicit example Jupyter notebooks for Garak might not be immediately apparent in the provided snippets, the tool's functionality and Python foundation make it amenable to use within such an environment for comprehensive LLM security testing. Users are encouraged to consult the official documentation and community resources for more detailed usage examples and potential notebook integrations.  
9.  Monitoring LLM Applications for Security Vulnerabilities: Jupyter Notebook Demonstrations:  
   Continuous monitoring of LLM applications in production is essential for detecting and responding to security vulnerabilities and other issues in real-time.32 Jupyter notebooks are proving to be valuable for demonstrating practical techniques and tools for this critical task. The Langfuse documentation provides a Jupyter notebook example (langfuse.com/docs/security/example-python) that specifically demonstrates how to monitor LLM security using the Langfuse platform in conjunction with security libraries like llm-guard.32 This notebook offers hands-on examples covering various security concerns, including detecting and blocking banned topics (like violence), anonymizing and deanonymizing Personally Identifiable Information (PII), using multiple security scanners in a support chat scenario, scanning the LLM's output for safety and quality, and detecting prompt injection attacks (even sophisticated ones like the "Grandma trick" using integration with Lakera Guard).32 This resource directly addresses the need for practical demonstrations of LLM security monitoring in a Jupyter environment \[Insight 1\]. The notebook's integration of multiple security libraries underscores the importance of a layered defense strategy \[Insight 2\], and its use of Langfuse highlights the value of observability platforms in gaining insights into and improving LLM security posture \[Insight 3\]. LangKit (github.com/whylabs/langkit) is another open-source toolkit designed for monitoring LLMs, offering a wide array of metrics related to text quality, relevance, security (including jailbreaks, prompt injection, hallucinations, and refusals), privacy, sentiment, and toxicity.33 The LangKit documentation mentions an example Jupyter notebook that allows users to experience the toolkit firsthand.33 LangKit's comprehensive set of metrics enables a nuanced understanding of LLM behavior and potential security risks \[Insight 1\], and its integration with whylogs facilitates effective logging and analysis of these metrics \[Insight 2\]. While not explicitly featuring Jupyter notebooks in the snippet, OpenLLMTelemetry (github.com/whylabs/openllmtelemetry) provides Open Telemetry integration for LLMs, enabling tracing and monitoring of LLM-based applications.34 The Python usage examples provided could be readily adapted into a Jupyter notebook environment. This library offers a standardized way to collect telemetry data, enhancing observability \[Insight 1\]. Additionally, the repository github.com/ArslanKAS/Quality-and-Safety-for-LLM-Applications points to a DeepLearning.AI short course focused on monitoring LLM systems for both quality and safety.35 While not a Jupyter notebook itself, the course likely includes practical exercises or resources that might involve notebooks. This highlights the importance of a holistic approach to monitoring that considers both security and overall performance \[Insight 1\]. In conclusion, Jupyter notebooks are becoming a key tool for demonstrating practical methods and the use of specialized libraries like Langfuse and LangKit for monitoring LLM applications in production to effectively detect and respond to security vulnerabilities.  
11. Conclusion:  
    The analysis of the provided research material reveals a rich and growing ecosystem of Jupyter notebook resources dedicated to understanding and demonstrating the security vulnerabilities of Large Language Models. Across the spectrum of prompt hacking techniques, including prompt injection, data and prompt leaking, and jailbreaking, numerous repositories and articles highlight the use of Jupyter notebooks for both theoretical exploration and practical experimentation.2 Similarly, the domain of backdoor attacks, encompassing training data poisoning and the creation and triggering of backdoors, is well-represented with Jupyter notebook examples that illustrate the mechanisms and potential impacts of these insidious threats.18 Furthermore, defensive measures like red teaming are increasingly leveraging the interactive capabilities of Jupyter notebooks, with frameworks and automated techniques being demonstrated within this environment.27 While the Garak framework is primarily a command-line tool, its integration and use within Jupyter notebooks for comprehensive LLM security testing are feasible and documented.30 Finally, the critical aspect of monitoring LLM applications in production for security vulnerabilities is supported by practical Jupyter notebook demonstrations using tools like Langfuse and LangKit.32 The increasing availability and sophistication of these resources underscore a growing awareness and concerted effort within the research and development community to address the unique security challenges posed by LLMs \[Insight 1\]. The prevalence of open-source platforms like GitHub as the primary source for these materials highlights the collaborative nature of this rapidly evolving field, fostering the sharing of knowledge and tools essential for building more secure and reliable LLM-powered applications \[Insight 2\]. It is recommended that users seeking hands-on experience with LLM security vulnerabilities and defenses explore the linked GitHub repositories and documentation to access these valuable Jupyter notebook resources and further their understanding in this critical domain.

**Table 1: Summary of Jupyter Notebook Resources for LLM Security Vulnerabilities**

| Vulnerability/Defense Category | Repository/Resource Name | Link to Repository/Resource | Description of Relevant Jupyter Notebooks (if explicitly mentioned or strongly implied) |
| :---- | :---- | :---- | :---- |
| Prompt Injection | github.com/sinanw/llm-security-prompt-injection | [https://github.com/sinanw/llm-security-prompt-injection](https://github.com/sinanw/llm-security-prompt-injection) | Contains notebooks analyzing prompt classification using ML and LLMs. |
| Prompt Injection | github.com/greshake/llm-security | [https://github.com/greshake/llm-security](https://github.com/greshake/llm-security) | Likely contains notebooks demonstrating indirect prompt injection attacks in the scenarios folder, given Jupyter Notebook is a primary language. |
| Data/Prompt Leaking | github.com/sternakt/prompt-leakage-probing | [https://github.com/sternakt/prompt-leakage-probing](https://github.com/sternakt/prompt-leakage-probing) | Includes three notebooks (low\_protection.ipynb, medium\_protection.ipynb, high\_protection.ipynb) for testing system prompt leakage. |
| Data/Prompt Leaking | github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini\_prompt\_attacks\_mitigation\_examples.ipynb | [https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini\_prompt\_attacks\_mitigation\_examples.ipynb](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_prompt_attacks_mitigation_examples.ipynb) | Demonstrates prompt attack mitigation, including data leaking with transformations. |
| Jailbreaking | github.com/cyberark/FuzzyAI | [https://github.com/cyberark/FuzzyAI](https://github.com/cyberark/FuzzyAI) | Includes interactive Jupyter notebooks under resources/notebooks/ for jailbreaking LLMs. |
| Jailbreaking | github.com/traghav/auto-redteam | [https://github.com/traghav/auto-redteam](https://github.com/traghav/auto-redteam) | Contains redteam.ipynb, a notebook demonstrating LLM red-teaming techniques using another LLM. |
| Backdoor Attacks (Poisoning) | github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/backdoor\_attack\_DGM.ipynb | ([https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/backdoor\_attack\_DGM.ipynb](https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/backdoor_attack_DGM.ipynb)) | Demonstrates training time backdoor attacks on Deep Generative Models. |
| Backdoor Attacks (Poisoning) | github.com/kohpangwei/data-poisoning-release | [https://github.com/kohpangwei/data-poisoning-release](https://github.com/kohpangwei/data-poisoning-release) | Contains Jupyter notebooks (67.4% of content) related to certified defenses for data poisoning attacks, potentially including attack demonstrations. |
| Backdoor Attacks (Poisoning) | github.com/reds-lab/Narcissus | [https://github.com/reds-lab/Narcissus](https://github.com/reds-lab/Narcissus) | Provides Narcissus.ipynb, a notebook demonstrating a clean-label backdoor attack. |
| Backdoor Attacks (Triggering) | github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/backdoor\_attack\_DGM.ipynb | ([https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/backdoor\_attack\_DGM.ipynb](https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/backdoor_attack_DGM.ipynb)) | Demonstrates the creation and triggering of backdoors in Deep Generative Models. |
| Backdoor Attacks (Triggering) | github.com/reds-lab/Narcissus | [https://github.com/reds-lab/Narcissus](https://github.com/reds-lab/Narcissus) | Includes Narcissus.ipynb, demonstrating the generation and use of the Narcissus backdoor trigger. |
| Red Teaming | github.com/traghav/auto-redteam | [https://github.com/traghav/auto-redteam](https://github.com/traghav/auto-redteam) | Contains redteam.ipynb, a notebook demonstrating LLM red-teaming techniques. |
| Production Monitoring | langfuse.com/docs/security/example-python | [https://langfuse.com/docs/security/example-python](https://langfuse.com/docs/security/example-python) | A Jupyter notebook demonstrating LLM security monitoring with Langfuse and llm-guard. |
| Production Monitoring | github.com/whylabs/langkit | [https://github.com/whylabs/langkit](https://github.com/whylabs/langkit) | Mentions an example Jupyter notebook for experiencing LangKit, a toolkit for LLM monitoring. |

#### **Works cited**

1. Avmb/adversarial\_MT\_prompt\_injection \- GitHub, accessed April 4, 2025, [https://github.com/Avmb/adversarial\_MT\_prompt\_injection](https://github.com/Avmb/adversarial_MT_prompt_injection)  
2. greshake/llm-security: New ways of breaking app ... \- GitHub, accessed April 4, 2025, [https://github.com/greshake/llm-security](https://github.com/greshake/llm-security)  
3. llm-security-prompt-injection/notebooks/3-llm-classification ... \- GitHub, accessed April 4, 2025, [https://github.com/sinanw/llm-security-prompt-injection/blob/main/notebooks/3-llm-classification-finetuned.ipynb](https://github.com/sinanw/llm-security-prompt-injection/blob/main/notebooks/3-llm-classification-finetuned.ipynb)  
4. sinanw/llm-security-prompt-injection: This project ... \- GitHub, accessed April 4, 2025, [https://github.com/sinanw/llm-security-prompt-injection](https://github.com/sinanw/llm-security-prompt-injection)  
5. LLMSecurity/HouYi: The automated prompt injection ... \- GitHub, accessed April 4, 2025, [https://github.com/LLMSecurity/HouYi](https://github.com/LLMSecurity/HouYi)  
6. LLM07:2025 System Prompt Leakage \- GitHub, accessed April 4, 2025, [https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/blob/main/2\_0\_vulns/LLM07\_SystemPromptLeakage.md](https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/blob/main/2_0_vulns/LLM07_SystemPromptLeakage.md)  
7. sternakt/prompt-leakage-probing \- GitHub, accessed April 4, 2025, [https://github.com/sternakt/prompt-leakage-probing](https://github.com/sternakt/prompt-leakage-probing)  
8. Code and datasets for the salesforce AI research paper on prompt leakage and multi-turn threats against LLMs \- GitHub, accessed April 4, 2025, [https://github.com/salesforce/prompt-leakage](https://github.com/salesforce/prompt-leakage)  
9. generative-ai/gemini/responsible-ai ... \- GitHub, accessed April 4, 2025, [https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini\_prompt\_attacks\_mitigation\_examples.ipynb](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_prompt_attacks_mitigation_examples.ipynb)  
10. Jailbreaking Every LLM With One Simple Click \- CyberArk, accessed April 4, 2025, [https://www.cyberark.com/resources/threat-research-blog/jailbreaking-every-llm-with-one-simple-click/](https://www.cyberark.com/resources/threat-research-blog/jailbreaking-every-llm-with-one-simple-click/)  
11. Exposing Jailbreak Vulnerabilities in LLM Applications with ARTKIT \- Medium, accessed April 4, 2025, [https://medium.com/data-science/exposing-jailbreak-vulnerabilities-in-llm-applications-with-artkit-d2df5f56ece8](https://medium.com/data-science/exposing-jailbreak-vulnerabilities-in-llm-applications-with-artkit-d2df5f56ece8)  
12. cyberark/FuzzyAI: A powerful tool for automated LLM ... \- GitHub, accessed April 4, 2025, [https://github.com/cyberark/FuzzyAI](https://github.com/cyberark/FuzzyAI)  
13. Jailbreaking Black Box Large Language Models in Twenty Queries \- GitHub, accessed April 4, 2025, [https://github.com/patrickrchao/JailbreakingLLMs](https://github.com/patrickrchao/JailbreakingLLMs)  
14. JailbreakBench: An Open Robustness Benchmark for Jailbreaking Language Models \[NeurIPS 2024 Datasets and Benchmarks Track\] \- GitHub, accessed April 4, 2025, [https://github.com/JailbreakBench/jailbreakbench](https://github.com/JailbreakBench/jailbreakbench)  
15. Code for Findings-EMNLP 2023 paper: Multi-step Jailbreaking Privacy Attacks on ChatGPT \- GitHub, accessed April 4, 2025, [https://github.com/HKUST-KnowComp/LLM-Multistep-Jailbreak](https://github.com/HKUST-KnowComp/LLM-Multistep-Jailbreak)  
16. THUYimingLi/backdoor-learning-resources: A list of ... \- GitHub, accessed April 4, 2025, [https://github.com/THUYimingLi/backdoor-learning-resources](https://github.com/THUYimingLi/backdoor-learning-resources)  
17. Offensive AI Compilation \- Your awesome title, accessed April 4, 2025, [https://jiep.github.io/offensive-ai-compilation/](https://jiep.github.io/offensive-ai-compilation/)  
18. adversarial-robustness-toolbox/notebooks/backdoor\_attack\_DGM ..., accessed April 4, 2025, [https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/backdoor\_attack\_DGM.ipynb](https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/backdoor_attack_DGM.ipynb)  
19. bboylyg/BackdoorLLM: BackdoorLLM: A Comprehensive ... \- GitHub, accessed April 4, 2025, [https://github.com/bboylyg/BackdoorLLM](https://github.com/bboylyg/BackdoorLLM)  
20. kohpangwei/data-poisoning-release \- GitHub, accessed April 4, 2025, [https://github.com/kohpangwei/data-poisoning-release](https://github.com/kohpangwei/data-poisoning-release)  
21. THUYimingLi/BackdoorBox: The open-sourced Python ... \- GitHub, accessed April 4, 2025, [https://github.com/THUYimingLi/BackdoorBox](https://github.com/THUYimingLi/BackdoorBox)  
22. tamingLLMs/tamingllms/notebooks/safety.ipynb at master ... \- GitHub, accessed April 4, 2025, [https://github.com/souzatharsis/tamingLLMs/blob/master/tamingllms/notebooks/safety.ipynb](https://github.com/souzatharsis/tamingLLMs/blob/master/tamingllms/notebooks/safety.ipynb)  
23. reds-lab/Narcissus: The official implementation of the CCS ... \- GitHub, accessed April 4, 2025, [https://github.com/reds-lab/Narcissus](https://github.com/reds-lab/Narcissus)  
24. ebagdasa/backdoors101: Backdoors Framework for Deep ... \- GitHub, accessed April 4, 2025, [https://github.com/ebagdasa/backdoors101](https://github.com/ebagdasa/backdoors101)  
25. confident-ai/deepteam: The LLM Red Teaming Framework \- GitHub, accessed April 4, 2025, [https://github.com/confident-ai/deepteam](https://github.com/confident-ai/deepteam)  
26. Improbable-AI/curiosity\_redteam: Official implementation of ICLR'24 paper, "Curiosity-driven Red Teaming for Large Language Models" (https://openreview.net/pdf?id=4KqkizXgXU) \- GitHub, accessed April 4, 2025, [https://github.com/Improbable-AI/curiosity\_redteam](https://github.com/Improbable-AI/curiosity_redteam)  
27. traghav/auto-redteam: Redteaming LLMs using other LLMs \- GitHub, accessed April 4, 2025, [https://github.com/traghav/auto-redteam](https://github.com/traghav/auto-redteam)  
28. NotSkynet/automated-interpretability-LLM-Red-Teaming \- GitHub, accessed April 4, 2025, [https://github.com/NotSkynet/automated-interpretability-LLM-Red-Teaming](https://github.com/NotSkynet/automated-interpretability-LLM-Red-Teaming)  
29. NVIDIA/garak: the LLM vulnerability scanner \- GitHub, accessed April 4, 2025, [https://github.com/NVIDIA/garak](https://github.com/NVIDIA/garak)  
30. Mastering LLM Security: A Deep Dive into Garak Vulnerability Scanner \- Medium, accessed April 4, 2025, [https://medium.com/@kachwalla64/mastering-llm-security-a-deep-dive-into-garak-vulnerability-scanner-a1274003aa47](https://medium.com/@kachwalla64/mastering-llm-security-a-deep-dive-into-garak-vulnerability-scanner-a1274003aa47)  
31. garak/FAQ.md at main \- GitHub, accessed April 4, 2025, [https://github.com/NVIDIA/garak/blob/main/FAQ.md](https://github.com/NVIDIA/garak/blob/main/FAQ.md)  
32. Example: Monitoring LLM Security \- Langfuse, accessed April 4, 2025, [https://langfuse.com/docs/security/example-python](https://langfuse.com/docs/security/example-python)  
33. whylabs/langkit: LangKit: An open-source toolkit for ... \- GitHub, accessed April 4, 2025, [https://github.com/whylabs/langkit](https://github.com/whylabs/langkit)  
34. whylabs/openllmtelemetry: Open LLM Telemetry package \- GitHub, accessed April 4, 2025, [https://github.com/whylabs/openllmtelemetry](https://github.com/whylabs/openllmtelemetry)  
35. ArslanKAS/Quality-and-Safety-for-LLM-Applications \- GitHub, accessed April 4, 2025, [https://github.com/ArslanKAS/Quality-and-Safety-for-LLM-Applications](https://github.com/ArslanKAS/Quality-and-Safety-for-LLM-Applications)